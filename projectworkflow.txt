---------------------- setting up project structure ------------

1. create a virtual environment named 'atlas' : conda create -n atlas python=3.10
2. Activate the atlas env : conda activate atlas
3. pip install cookiecutter : to build project structure, the have multiple templetes on github i guess
   so it just create files and build project struture.
4. "cookiecutter -c v1 https://github.com/drivendata/cookiecutter-data-science" : to copy the templete (skip aws things we will setup this manually)
5. cut all files from newly created and past on root folder
6. rename src.models to src.model becasue we already have model in root so its like just to make the mind clear.
7. git push origin main


---------------------- setup MLflow on dagshub & local DVC initialization -------------

8. Go to: https://dagshub.com/dashboard
9. Create > New Repo > Connect a repo > (Github) Connect > Select your repo > Connect
10. Copy experiment tracking url and code snippet. (Also try: Go To MLFlow UI)
11. pip install dagshub && pip install mlflow

12. Now tracking experiment with 3 models in a loop using MLfow and dagsub,  file: exp1.ipynb

13. Also Tracking data in our local system later will do on aws s3 so, : "pip install dvc" & "dvc init"
14. create a new local_s3 folder for dvc, on root dir
15  Initialize that local_s3 for dvc : "dvc remote add -d local_system local_s3"
16. Add local_s3 in .gitignore

17. git should track the dvc data id or pointers so : "git add .dvc .dvcignore"  && "git commit -m "Initialize DVC" && "git push origain main"

